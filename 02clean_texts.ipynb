{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "cleaning texts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "#helper functions for text cleaning\n",
    "############################################################\n",
    "\n",
    "#borrowed from jabryden's code\n",
    "stop_words = set([\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \n",
    "                  \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "                  \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "                  \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \n",
    "                  \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n",
    "                  \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
    "                  \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\",\n",
    "                  \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \n",
    "                  \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\",\n",
    "                  \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \n",
    "                  \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\",\n",
    "                  \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\",\n",
    "                  \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\",\n",
    "                  \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\",\n",
    "                  \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", 'rt'])\n",
    "\n",
    "punct = \"\"\"!\"#$%&'()*+, -./:;<=>?[\\]^_`{|}~??£??????\"\"\"\n",
    "\n",
    "exclusions = lambda x : [(x in stop_words), #drop stopwords\n",
    "                         (x[0].isnumeric())] #drop numbers\n",
    "\n",
    "deEmojify_pattern = re.compile(pattern = \"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       \"]+\", flags = re.UNICODE)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = deEmojify_pattern.sub(r'',text) #remove emojis, etc.\n",
    "    text = re.sub(r'#\\w+', r'', text) #remove hashtags\n",
    "    text = re.sub(r'@\\w+', r'', text) #remove username mentions\n",
    "    text = re.sub(r'(RT :)', r'', text) #remove RTs\n",
    "    text = re.sub(r'http\\S+', '', text) #remove URLs\n",
    "#     text = re.sub(r'-', r' -', text) #split hyphenated words\n",
    "\n",
    "    words = [word.strip(punct).lower() for word in text.split()]\n",
    "    words = [word for word in words if (word!='') and (not any(exclusions(word)))]\n",
    "    if len(words)==0 or words==['']: return None\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "############################################################\n",
    "# load data\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "print('loading data')\n",
    "#load metadata files\n",
    "# data_path = '/geode2/home/u040/jmbollen/Carbonate/parliament/data/'\n",
    "data_path = './data/'\n",
    "commons_speeches = pd.read_parquet(data_path+'commons_speeches.parquet')\n",
    "mp_tweets = pd.read_parquet(data_path+'mp_tweets.parquet')\n",
    "urls = pd.read_parquet(data_path+'urls.parquet')\n",
    "urls['domain'] = urls['clean_url'].apply(lambda x: (urlparse(x).netloc).replace('www.',''))\n",
    "\n",
    "mp_tweets['text_type']='tweet'\n",
    "urls['text_type']='url'\n",
    "commons_speeches['text_type']='commons_speech'\n",
    "\n",
    "commons_speeches = commons_speeches.rename(columns={'commons_speech_id':'text_id'})\n",
    "urls = urls.rename(columns={'url_rid':'text_id'})\n",
    "mp_tweets = mp_tweets.rename(columns={'tweet_id':'text_id'})\n",
    "\n",
    "all_texts = pd.concat([mp_tweets,commons_speeches,urls])\n",
    "\n",
    "del commons_speeches; del urls; del mp_tweets\n",
    "\n",
    "\n",
    "print('cleaning texts')\n",
    "all_texts['text'] = all_texts['text'].apply(clean_text)\n",
    "\n",
    "all_texts = all_texts.dropna(subset=['text']\n",
    "                    ).drop_duplicates(subset=['text_id']\n",
    "                    ).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mp_name</th>\n",
       "      <th>text_id</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>n_likes</th>\n",
       "      <th>n_replies</th>\n",
       "      <th>n_retweets</th>\n",
       "      <th>n_quotes</th>\n",
       "      <th>party</th>\n",
       "      <th>...</th>\n",
       "      <th>sum_shares</th>\n",
       "      <th>sum_likes</th>\n",
       "      <th>sum_loves</th>\n",
       "      <th>sum_hahas</th>\n",
       "      <th>sum_wows</th>\n",
       "      <th>sum_sorrys</th>\n",
       "      <th>sum_angers</th>\n",
       "      <th>sum_comments</th>\n",
       "      <th>sum_share_without_clicks</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gary streeter</td>\n",
       "      <td>881832084603633664</td>\n",
       "      <td>2017-07-03 11:08:47</td>\n",
       "      <td>compelling case pay increase nurses made conse...</td>\n",
       "      <td>4785228995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Con</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gary streeter</td>\n",
       "      <td>881845410364628993</td>\n",
       "      <td>2017-07-03 12:01:44</td>\n",
       "      <td>glad see selected best ever photo could auditi...</td>\n",
       "      <td>4785228995</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Con</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gary streeter</td>\n",
       "      <td>881925006112018433</td>\n",
       "      <td>2017-07-03 17:18:01</td>\n",
       "      <td>assured intruders removed asap excuse breaking...</td>\n",
       "      <td>4785228995</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Con</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gary streeter</td>\n",
       "      <td>881925078564392961</td>\n",
       "      <td>2017-07-03 17:18:18</td>\n",
       "      <td>need help us create list buildings places tell...</td>\n",
       "      <td>4785228995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Con</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gary streeter</td>\n",
       "      <td>881946858318422018</td>\n",
       "      <td>2017-07-03 18:44:51</td>\n",
       "      <td>proud species champion horrid ground weaver sp...</td>\n",
       "      <td>4785228995</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Con</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         mp_name             text_id                time  \\\n",
       "0  gary streeter  881832084603633664 2017-07-03 11:08:47   \n",
       "1  gary streeter  881845410364628993 2017-07-03 12:01:44   \n",
       "2  gary streeter  881925006112018433 2017-07-03 17:18:01   \n",
       "3  gary streeter  881925078564392961 2017-07-03 17:18:18   \n",
       "4  gary streeter  881946858318422018 2017-07-03 18:44:51   \n",
       "\n",
       "                                                text     user_id  n_likes  \\\n",
       "0  compelling case pay increase nurses made conse...  4785228995      0.0   \n",
       "1  glad see selected best ever photo could auditi...  4785228995      5.0   \n",
       "2  assured intruders removed asap excuse breaking...  4785228995      4.0   \n",
       "3  need help us create list buildings places tell...  4785228995      0.0   \n",
       "4  proud species champion horrid ground weaver sp...  4785228995     32.0   \n",
       "\n",
       "   n_replies  n_retweets  n_quotes party  ... sum_shares sum_likes sum_loves  \\\n",
       "0        0.0        13.0       0.0   Con  ...        NaN       NaN       NaN   \n",
       "1        5.0         2.0       2.0   Con  ...        NaN       NaN       NaN   \n",
       "2        0.0         0.0       1.0   Con  ...        NaN       NaN       NaN   \n",
       "3        0.0       260.0       0.0   Con  ...        NaN       NaN       NaN   \n",
       "4        4.0        10.0       3.0   Con  ...        NaN       NaN       NaN   \n",
       "\n",
       "  sum_hahas  sum_wows  sum_sorrys  sum_angers  sum_comments  \\\n",
       "0       NaN       NaN         NaN         NaN           NaN   \n",
       "1       NaN       NaN         NaN         NaN           NaN   \n",
       "2       NaN       NaN         NaN         NaN           NaN   \n",
       "3       NaN       NaN         NaN         NaN           NaN   \n",
       "4       NaN       NaN         NaN         NaN           NaN   \n",
       "\n",
       "   sum_share_without_clicks  domain  \n",
       "0                       NaN     NaN  \n",
       "1                       NaN     NaN  \n",
       "2                       NaN     NaN  \n",
       "3                       NaN     NaN  \n",
       "4                       NaN     NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3481563 entries, 0 to 3481562\n",
      "Data columns (total 27 columns):\n",
      " #   Column                    Dtype         \n",
      "---  ------                    -----         \n",
      " 0   mp_name                   object        \n",
      " 1   text_id                   object        \n",
      " 2   time                      datetime64[ns]\n",
      " 3   text                      object        \n",
      " 4   user_id                   object        \n",
      " 5   n_likes                   float64       \n",
      " 6   n_replies                 float64       \n",
      " 7   n_retweets                float64       \n",
      " 8   n_quotes                  float64       \n",
      " 9   party                     object        \n",
      " 10  text_type                 object        \n",
      " 11  constituency              object        \n",
      " 12  job                       object        \n",
      " 13  clean_url                 object        \n",
      " 14  count                     float64       \n",
      " 15  sum_views                 float64       \n",
      " 16  sum_clicks                float64       \n",
      " 17  sum_shares                float64       \n",
      " 18  sum_likes                 float64       \n",
      " 19  sum_loves                 float64       \n",
      " 20  sum_hahas                 float64       \n",
      " 21  sum_wows                  float64       \n",
      " 22  sum_sorrys                float64       \n",
      " 23  sum_angers                float64       \n",
      " 24  sum_comments              float64       \n",
      " 25  sum_share_without_clicks  float64       \n",
      " 26  domain                    object        \n",
      "dtypes: datetime64[ns](1), float64(16), object(10)\n",
      "memory usage: 717.2+ MB\n"
     ]
    }
   ],
   "source": [
    "all_texts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts.to_parquet(data_path + 'all_texts_cleaned.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
